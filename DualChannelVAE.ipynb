{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T06:25:28.737253Z",
     "start_time": "2025-08-24T06:25:28.712420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from modules.SaveOutputs import save_reconstructions\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "# The dimension of the latent space, a key hyperparameter for VAEs.\n",
    "LATENT_DIM = 128\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# --- 1. Define the VAE Model ---\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # --- New Encoder Network with Split Paths ---\n",
    "        # The encoder starts with a shared convolutional block.\n",
    "        self.initial_encoder = nn.Sequential(\n",
    "            # Input: [1, 64, 64]\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # Output: [32, 32, 32]\n",
    "        )\n",
    "\n",
    "        # Path 1: A simple convolutional path\n",
    "        self.path1 = nn.Sequential(\n",
    "            # Input: [32, 32, 32]\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # Output: [64, 16, 16]\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # Output: [128, 8, 8]\n",
    "        )\n",
    "\n",
    "        # Path 2: A \"VGG-like\" path with more layers for finer feature extraction\n",
    "        self.path2 = nn.Sequential(\n",
    "            # Input: [32, 32, 32]\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), # VGG-like conv\n",
    "            nn.ReLU(),\n",
    "            # Output: [64, 32, 32]\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # Output: [128, 16, 16]\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # Output: [256, 8, 8]\n",
    "        )\n",
    "\n",
    "        # A final convolutional layer to merge the features from both paths\n",
    "        # Concatenated output will be [128 + 256, 8, 8] = [384, 8, 8]\n",
    "        self.final_encoder = nn.Sequential(\n",
    "            nn.Conv2d(384, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # Output: [512, 4, 4]\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Linear layers for latent space, adjusted for the new merged feature size\n",
    "        self.fc_mu = nn.Linear(512 * 4 * 4, LATENT_DIM)\n",
    "        self.fc_log_var = nn.Linear(512 * 4 * 4, LATENT_DIM)\n",
    "\n",
    "        # --- Decoder Network ---\n",
    "        # The decoder now takes a latent vector and reconstructs the image.\n",
    "        self.decoder_fc = nn.Linear(LATENT_DIM, 512 * 4 * 4)\n",
    "\n",
    "        # We use a series of transposed convolutional layers (ConvTranspose2d) to upsample.\n",
    "        # This part of the decoder is now larger to match the new encoder output.\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Input: [512, 4, 4]\n",
    "            nn.Unflatten(1, (512, 4, 4)),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # Output: [256, 8, 8]\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # Output: [128, 16, 16]\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # Output: [64, 32, 32]\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            # Final activation to output pixel values in the [0, 1] range.\n",
    "            nn.Sigmoid()\n",
    "            # Output: [1, 64, 64]\n",
    "        )\n",
    "\n",
    "    # The reparameterization trick allows us to backpropagate through the sampling process.\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)  # standard deviation\n",
    "        epsilon = torch.randn_like(std) # sample from a standard normal distribution\n",
    "        return mu + epsilon * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial shared encoding\n",
    "        h = self.initial_encoder(x)\n",
    "\n",
    "        # Split into two paths\n",
    "        h1 = self.path1(h)\n",
    "        h2 = self.path2(h)\n",
    "\n",
    "        # Concatenate features from both paths along the channel dimension\n",
    "        h_merged = torch.cat((h1, h2), dim=1)\n",
    "\n",
    "        # Final shared encoding and flatten\n",
    "        h_final = self.final_encoder(h_merged)\n",
    "\n",
    "        mu = self.fc_mu(h_final)\n",
    "        log_var = self.fc_log_var(h_final)\n",
    "\n",
    "        # Sample a point from the latent space using the reparameterization trick\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "\n",
    "        # Decode the latent vector back into an image\n",
    "        reconstructed_x = self.decoder(self.decoder_fc(z))\n",
    "\n",
    "        return reconstructed_x, mu, log_var"
   ],
   "id": "c9aba7699b7688cc",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T06:19:16.869223Z",
     "start_time": "2025-08-24T06:19:16.861225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 2. Define the VAE Loss Function ---\n",
    "# The VAE loss is a combination of two components:\n",
    "# 1. Reconstruction Loss: How well the VAE reconstructs the input image.\n",
    "# 2. KL Divergence Loss: A regularization term that keeps the latent space distribution\n",
    "#    close to a standard normal distribution.\n",
    "def vae_loss(reconstructed_x, x, mu, log_var):\n",
    "    # Binary Cross-Entropy (BCE) for the reconstruction loss.\n",
    "    # We use reduction='sum' to be consistent with the KL divergence term.\n",
    "    reconstruction_loss = F.binary_cross_entropy(reconstructed_x, x, reduction='sum')\n",
    "\n",
    "    # KL Divergence between the learned latent distribution and a standard normal distribution.\n",
    "    # The formula is 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    kl_divergence_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "    return reconstruction_loss + kl_divergence_loss\n",
    "\n",
    "# --- 3. Data Loading and Preprocessing ---\n",
    "# We use the MNIST dataset and resize the images to 64x64 pixels.\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T06:19:20.891560Z",
     "start_time": "2025-08-24T06:19:20.188258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()  # convert to tensor & scale to [0,1]\n",
    "])\n",
    "dataset = datasets.ImageFolder(root=\"G:\\Temp\", transform=transform)\n",
    "train_test_split_var = 0.99\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [int(train_test_split_var*len(dataset)), len(dataset) - int(train_test_split_var*len(dataset))])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=2, pin_memory=True)\n",
    "n_train = len(train_loader.dataset)"
   ],
   "id": "2509afd64be2c2ba",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T06:19:22.490510Z",
     "start_time": "2025-08-24T06:19:22.388360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "    # Instantiate the VAE model and move it to the device\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Training loop\n"
   ],
   "id": "6745763feadf4577",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (initial_encoder): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (path1): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (path2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (final_encoder): Sequential(\n",
       "    (0): Conv2d(384, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (fc_mu): Linear(in_features=8192, out_features=128, bias=True)\n",
       "  (fc_log_var): Linear(in_features=8192, out_features=128, bias=True)\n",
       "  (decoder_fc): Linear(in_features=128, out_features=8192, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Unflatten(dim=1, unflattened_size=(512, 4, 4))\n",
       "    (1): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (2): ReLU()\n",
       "    (3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (8): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T07:33:08.228625Z",
     "start_time": "2025-08-24T07:09:01.599215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for GPU availability\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (images, _) in tqdm(enumerate(train_loader)):\n",
    "            # Reshape and move images to the correct device\n",
    "        images = images.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "        reconstructed_images, mu, log_var = model(images)\n",
    "        loss = vae_loss(reconstructed_images, images, mu, log_var)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_loss:.4f}\")\n",
    "    save_reconstructions(model=model, x=images, out_dir=\"G:\\Python\\VAE-latent-space-experiment\\outputs\", step = epoch+5, device=device, variant=\"\")\n",
    "\n",
    "print(\"Training finished!\")\n"
   ],
   "id": "53175ee0a0f16be6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1567it [01:22, 18.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 6236.5850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1567it [01:23, 18.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 6235.7465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1567it [01:21, 19.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 6234.2730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1567it [01:21, 19.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Loss: 6233.7973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1567it [01:28, 17.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Loss: 6232.7996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1567it [01:25, 18.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/5], Loss: 6232.1588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1567it [01:26, 18.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/5], Loss: 6231.6171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1567it [01:25, 18.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/5], Loss: 6230.9793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1567it [01:26, 18.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/5], Loss: 6230.5429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1567it [01:28, 17.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/5], Loss: 6230.0408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1567it [01:29, 17.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/5], Loss: 6229.4530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1567it [01:28, 17.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/5], Loss: 6229.1816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1567it [01:30, 17.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/5], Loss: 6228.7009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1567it [01:28, 17.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/5], Loss: 6228.2270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1567it [01:28, 17.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/5], Loss: 6228.0413\n",
      "Training finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T06:18:43.491066Z",
     "start_time": "2025-08-24T06:18:43.222974Z"
    }
   },
   "cell_type": "code",
   "source": "\n",
   "id": "c0f790fd8b888c03",
   "outputs": [],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
